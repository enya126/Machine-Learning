{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## Enya Liu\n",
    "\n",
    "## Logistic Regression without using any libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers here if you wish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your allowed to use only the above libraries that are imported. No other libs should be used in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset \n",
    "\n",
    "In this Assignment we will work with some patients dataset. \n",
    "\n",
    "We have access to 303 patients data. The features are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  \\\n",
       "0     63    1       typical     145   233    1        2    150      0   \n",
       "1     67    1  asymptomatic     160   286    0        2    108      1   \n",
       "2     67    1  asymptomatic     120   229    0        2    129      1   \n",
       "3     37    1    nonanginal     130   250    0        0    187      0   \n",
       "4     41    0    nontypical     130   204    0        2    172      0   \n",
       "..   ...  ...           ...     ...   ...  ...      ...    ...    ...   \n",
       "298   45    1       typical     110   264    0        0    132      0   \n",
       "299   68    1  asymptomatic     144   193    1        0    141      0   \n",
       "300   57    1  asymptomatic     130   131    0        0    115      1   \n",
       "301   57    0    nontypical     130   236    0        2    174      0   \n",
       "302   38    1    nonanginal     138   175    0        0    173      0   \n",
       "\n",
       "     Oldpeak  Slope   Ca        Thal Target  \n",
       "0        2.3      3  0.0       fixed     No  \n",
       "1        1.5      2  3.0      normal    Yes  \n",
       "2        2.6      2  2.0  reversable    Yes  \n",
       "3        3.5      3  0.0      normal     No  \n",
       "4        1.4      1  0.0      normal     No  \n",
       "..       ...    ...  ...         ...    ...  \n",
       "298      1.2      2  0.0  reversable    Yes  \n",
       "299      3.4      2  2.0  reversable    Yes  \n",
       "300      1.2      2  1.0  reversable    Yes  \n",
       "301      0.0      2  1.0      normal    Yes  \n",
       "302      0.0      1  NaN      normal     No  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "heart_df = pd.read_csv(\"Heart.csv\")\n",
    "# heart_df = pd.read_csv(r'C:\\Users\\Ley\\Desktop\\Heart.csv')\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age:** The personâ€™s age in years\n",
    "\n",
    "**Sex:** The personâ€™s sex (1 = male, 0 = female)\n",
    "\n",
    "**ChestPain:** chest pain type\n",
    "\n",
    "* Value 0: asymptomatic\n",
    "* Value 1: atypical angina\n",
    "* Value 2: non-anginal pain\n",
    "* Value 3: typical angina\n",
    "\n",
    "**RestBP:** The personâ€™s resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "**Chol:** The personâ€™s cholesterol measurement in mg/dl\n",
    "\n",
    "**Fbs:** The personâ€™s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "* Value 0: showing probable or definite left ventricular hypertrophy by Estesâ€™ criteria\n",
    "* Value 1: normal\n",
    "* Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "**RestECG:** The personâ€™s maximum heart rate achieved\n",
    "\n",
    "**MaxHR:** Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "**Oldpeak:** ST depression induced by exercise relative to rest (â€˜STâ€™ relates to positions on the ECG plot. See more here)\n",
    "\n",
    "**Slope:** the slope of the peak exercise ST segment â€” 0: downsloping; 1: flat; 2: upsloping\n",
    "\n",
    "* 0: downsloping; \n",
    "* 1: flat; \n",
    "* 2: upsloping\n",
    "\n",
    "**Ca:** The number of major vessels (0â€“3)\n",
    "\n",
    "**Thal:** A blood disorder called thalassemia Value 0: NULL (dropped from the dataset previously\n",
    "\n",
    "* Value 1: fixed defect (no blood flow in some part of the heart)\n",
    "* Value 2: normal blood flow\n",
    "* Value 3: reversible defect (a blood flow is observed but it is not normal)\n",
    "\n",
    "**Target:** Heart disease (1 = no, 0= yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 1 (4 points)\n",
    "We want to use logistic regerssion to perdict if the patients will have heart problems or not. The column \"Target\" in our datasets includes data about heart diseases. If the patient had heart disease we have a 1 and if not a zero. \n",
    "\n",
    "Prepare your data set for predicting heart disease (\"Target\" column) out of 3 features:\n",
    "\n",
    "* Age of the patient (Column **\"Age\"**)\n",
    "* Gender of the patient (male or female - Column **\"Sex\"**)\n",
    "* Cholestrol level of the patient (Column **\"Chol\"**) \n",
    "\n",
    "\n",
    "Split your data into 80% traning data and 20% test data, and implement logistic regression model without using any libs than imported above. \n",
    "\n",
    "* Do maximum **100 iterations**\n",
    "* Use a very small learning rate for checking your GD implementation. \n",
    "* Your are allowed to use your choice of learning rate, like using 0.0001, 0.001 or 0.01 or 0.1 or higher. \n",
    "* Visualize your costs. \n",
    "* No need to add an y-intercept in this task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is:  0.6931471805599453\n",
      "Cost is:  0.6911184331775377\n",
      "Cost is:  0.6900362632202808\n",
      "Cost is:  0.6895345748334202\n",
      "Cost is:  0.6892692145377417\n",
      "Cost is:  0.6891346512728148\n",
      "Cost is:  0.6890589700543245\n",
      "Cost is:  0.6890140977880899\n",
      "Cost is:  0.6889837977631849\n",
      "Cost is:  0.6889609380923364\n",
      "Cost is:  0.6889417173798884\n",
      "Cost is:  0.6889243662375394\n",
      "Cost is:  0.6889079787661342\n",
      "Cost is:  0.6888921208963703\n",
      "Cost is:  0.6888765745596712\n",
      "Cost is:  0.688861233527782\n",
      "Cost is:  0.6888460447081419\n",
      "Cost is:  0.6888309817929033\n",
      "Cost is:  0.6888160314433742\n",
      "Cost is:  0.6888011867684374\n",
      "Cost is:  0.6887864440332955\n",
      "Cost is:  0.6887718010604265\n",
      "Cost is:  0.6887572564420875\n",
      "Cost is:  0.6887428091491318\n",
      "Cost is:  0.6887284583428365\n",
      "Cost is:  0.6887142032788686\n",
      "Cost is:  0.6887000432626601\n",
      "Cost is:  0.6886859776255673\n",
      "Cost is:  0.6886720057144695\n",
      "Cost is:  0.6886581268857029\n",
      "Cost is:  0.6886443405027215\n",
      "Cost is:  0.6886306459344713\n",
      "Cost is:  0.6886170425549004\n",
      "Cost is:  0.6886035297424766\n",
      "Cost is:  0.6885901068800943\n",
      "Cost is:  0.6885767733549044\n",
      "Cost is:  0.688563528558293\n",
      "Cost is:  0.6885503718858025\n",
      "Cost is:  0.688537302737117\n",
      "Cost is:  0.6885243205160148\n",
      "Cost is:  0.6885114246303488\n",
      "Cost is:  0.688498614492011\n",
      "Cost is:  0.6884858895169093\n",
      "Cost is:  0.6884732491249371\n",
      "Cost is:  0.6884606927399483\n",
      "Cost is:  0.6884482197897296\n",
      "Cost is:  0.6884358297059737\n",
      "Cost is:  0.6884235219242534\n",
      "Cost is:  0.6884112958839953\n",
      "Cost is:  0.6883991510284533\n",
      "Cost is:  0.6883870868046831\n",
      "Cost is:  0.6883751026635163\n",
      "Cost is:  0.6883631980595356\n",
      "Cost is:  0.6883513724510477\n",
      "Cost is:  0.68833962530006\n",
      "Cost is:  0.688327956072254\n",
      "Cost is:  0.6883163642369617\n",
      "Cost is:  0.6883048492671396\n",
      "Cost is:  0.6882934106393455\n",
      "Cost is:  0.6882820478337127\n",
      "Cost is:  0.6882707603339265\n",
      "Cost is:  0.6882595476272007\n",
      "Cost is:  0.6882484092042518\n",
      "Cost is:  0.6882373445592773\n",
      "Cost is:  0.688226353189931\n",
      "Cost is:  0.6882154345972996\n",
      "Cost is:  0.6882045882858792\n",
      "Cost is:  0.6881938137635529\n",
      "Cost is:  0.6881831105415672\n",
      "Cost is:  0.6881724781345092\n",
      "Cost is:  0.6881619160602834\n",
      "Cost is:  0.6881514238400909\n",
      "Cost is:  0.6881410009984048\n",
      "Cost is:  0.6881306470629492\n",
      "Cost is:  0.6881203615646768\n",
      "Cost is:  0.6881101440377468\n",
      "Cost is:  0.6880999940195035\n",
      "Cost is:  0.6880899110504543\n",
      "Cost is:  0.6880798946742478\n",
      "Cost is:  0.6880699444376533\n",
      "Cost is:  0.6880600598905386\n",
      "Cost is:  0.6880502405858496\n",
      "Cost is:  0.6880404860795889\n",
      "Cost is:  0.6880307959307954\n",
      "Cost is:  0.6880211697015232\n",
      "Cost is:  0.6880116069568207\n",
      "Cost is:  0.6880021072647116\n",
      "Cost is:  0.6879926701961728\n",
      "Cost is:  0.6879832953251155\n",
      "Cost is:  0.6879739822283649\n",
      "Cost is:  0.6879647304856401\n",
      "Cost is:  0.6879555396795346\n",
      "Cost is:  0.687946409395496\n",
      "Cost is:  0.687937339221808\n",
      "Cost is:  0.6879283287495693\n",
      "Cost is:  0.6879193775726756\n",
      "Cost is:  0.6879104852878001\n",
      "Cost is:  0.6879016514943742\n",
      "Cost is:  0.6878928757945691\n",
      "Cost is:  0.6878841577932774\n",
      "Cost is:  0.6878754970980931\n",
      "Cost is:  0.6878668933192953\n",
      "Cost is:  0.6878583460698277\n",
      "Cost is:  0.6878498549652814\n",
      "Cost is:  0.6878414196238772\n",
      "Cost is:  0.6878330396664469\n",
      "Cost is:  0.6878247147164156\n",
      "Cost is:  0.6878164443997843\n",
      "Cost is:  0.6878082283451115\n",
      "Cost is:  0.6878000661834968\n",
      "Cost is:  0.6877919575485628\n",
      "Cost is:  0.6877839020764378\n",
      "Cost is:  0.6877758994057389\n",
      "Cost is:  0.687767949177555\n",
      "Cost is:  0.6877600510354297\n",
      "Cost is:  0.6877522046253446\n",
      "Cost is:  0.6877444095957026\n",
      "Cost is:  0.6877366655973114\n",
      "Cost is:  0.6877289722833665\n",
      "Cost is:  0.6877213293094357\n",
      "Cost is:  0.6877137363334427\n",
      "Cost is:  0.68770619301565\n",
      "Cost is:  0.6876986990186438\n",
      "Cost is:  0.6876912540073181\n",
      "Cost is:  0.6876838576488586\n",
      "Cost is:  0.687676509612727\n",
      "Cost is:  0.6876692095706451\n",
      "Cost is:  0.6876619571965802\n",
      "Cost is:  0.6876547521667289\n",
      "Cost is:  0.6876475941595018\n",
      "Cost is:  0.6876404828555092\n",
      "Cost is:  0.687633417937545\n",
      "Cost is:  0.6876263990905721\n",
      "Cost is:  0.6876194260017081\n",
      "Cost is:  0.6876124983602094\n",
      "Cost is:  0.6876056158574575\n",
      "Cost is:  0.6875987781869437\n",
      "Cost is:  0.6875919850442556\n",
      "Cost is:  0.6875852361270615\n",
      "Cost is:  0.6875785311350971\n",
      "Cost is:  0.6875718697701508\n",
      "Cost is:  0.6875652517360501\n",
      "Cost is:  0.6875586767386467\n",
      "Cost is:  0.6875521444858036\n",
      "Cost is:  0.6875456546873809\n",
      "Cost is:  0.687539207055222\n",
      "Cost is:  0.6875328013031401\n",
      "Cost is:  0.6875264371469046\n",
      "Cost is:  0.6875201143042282\n",
      "Cost is:  0.6875138324947523\n",
      "Cost is:  0.6875075914400349\n",
      "Cost is:  0.6875013908635377\n",
      "Cost is:  0.6874952304906117\n",
      "Cost is:  0.6874891100484852\n",
      "Cost is:  0.6874830292662508\n",
      "Cost is:  0.6874769878748523\n",
      "Cost is:  0.6874709856070722\n",
      "Cost is:  0.6874650221975197\n",
      "Cost is:  0.6874590973826163\n",
      "Cost is:  0.6874532109005856\n",
      "Cost is:  0.6874473624914396\n",
      "Cost is:  0.6874415518969663\n",
      "Cost is:  0.6874357788607187\n",
      "Cost is:  0.6874300431280012\n",
      "Cost is:  0.6874243444458586\n",
      "Cost is:  0.6874186825630642\n",
      "Cost is:  0.6874130572301065\n",
      "Cost is:  0.6874074681991794\n",
      "Cost is:  0.6874019152241694\n",
      "Cost is:  0.6873963980606441\n",
      "Cost is:  0.6873909164658404\n",
      "Cost is:  0.6873854701986534\n",
      "Cost is:  0.6873800590196253\n",
      "Cost is:  0.6873746826909337\n",
      "Cost is:  0.6873693409763799\n",
      "Cost is:  0.6873640336413787\n",
      "Cost is:  0.6873587604529465\n",
      "Cost is:  0.6873535211796911\n",
      "Cost is:  0.6873483155917999\n",
      "Cost is:  0.6873431434610295\n",
      "Cost is:  0.6873380045606956\n",
      "Cost is:  0.6873328986656605\n",
      "Cost is:  0.6873278255523242\n",
      "Cost is:  0.6873227849986131\n",
      "Cost is:  0.6873177767839698\n",
      "Cost is:  0.6873128006893421\n",
      "Cost is:  0.6873078564971731\n",
      "Cost is:  0.6873029439913914\n",
      "Cost is:  0.6872980629573997\n",
      "Cost is:  0.687293213182066\n",
      "Cost is:  0.6872883944537121\n",
      "Cost is:  0.6872836065621053\n",
      "Cost is:  0.687278849298447\n",
      "Cost is:  0.6872741224553633\n",
      "Cost is:  0.687269425826896\n",
      "Cost is:  0.6872647592084914\n",
      "Cost is:  0.6872601223969923\n",
      "Cost is:  0.6872555151906266\n",
      "Cost is:  0.6872509373889997\n",
      "Cost is:  0.6872463887930833\n",
      "Cost is:  0.6872418692052074\n",
      "Cost is:  0.6872373784290502\n",
      "Cost is:  0.6872329162696288\n",
      "Cost is:  0.6872284825332904\n",
      "Cost is:  0.6872240770277027\n",
      "Cost is:  0.6872196995618459\n",
      "Cost is:  0.687215349946002\n",
      "Cost is:  0.687211027991747\n",
      "Cost is:  0.6872067335119423\n",
      "Cost is:  0.6872024663207246\n",
      "Cost is:  0.687198226233498\n",
      "Cost is:  0.6871940130669258\n",
      "Cost is:  0.6871898266389204\n",
      "Cost is:  0.6871856667686362\n",
      "Cost is:  0.6871815332764598\n",
      "Cost is:  0.6871774259840029\n",
      "Cost is:  0.6871733447140924\n",
      "Cost is:  0.6871692892907634\n",
      "Cost is:  0.6871652595392498\n",
      "Cost is:  0.6871612552859767\n",
      "Cost is:  0.6871572763585525\n",
      "Cost is:  0.6871533225857596\n",
      "Cost is:  0.6871493937975479\n",
      "Cost is:  0.6871454898250255\n",
      "Cost is:  0.6871416105004513\n",
      "Cost is:  0.687137755657227\n",
      "Cost is:  0.6871339251298895\n",
      "Cost is:  0.6871301187541025\n",
      "Cost is:  0.6871263363666494\n",
      "Cost is:  0.6871225778054257\n",
      "Cost is:  0.6871188429094304\n",
      "Cost is:  0.6871151315187591\n",
      "Cost is:  0.6871114434745971\n",
      "Cost is:  0.6871077786192106\n",
      "Cost is:  0.6871041367959398\n",
      "Cost is:  0.6871005178491923\n",
      "Cost is:  0.6870969216244349\n",
      "Cost is:  0.6870933479681864\n",
      "Cost is:  0.6870897967280105\n",
      "Cost is:  0.6870862677525087\n",
      "Cost is:  0.6870827608913131\n",
      "Cost is:  0.68707927599508\n",
      "Cost is:  0.687075812915481\n",
      "Cost is:  0.6870723715051985\n",
      "Cost is:  0.6870689516179169\n",
      "Cost is:  0.6870655531083166\n",
      "Cost is:  0.6870621758320664\n",
      "Cost is:  0.6870588196458182\n",
      "Cost is:  0.6870554844071983\n",
      "Cost is:  0.6870521699748026\n",
      "Cost is:  0.6870488762081884\n",
      "Cost is:  0.6870456029678688\n",
      "Cost is:  0.6870423501153057\n",
      "Cost is:  0.6870391175129035\n",
      "Cost is:  0.6870359050240026\n",
      "Cost is:  0.6870327125128727\n",
      "Cost is:  0.6870295398447066\n",
      "Cost is:  0.6870263868856143\n",
      "Cost is:  0.6870232535026162\n",
      "Cost is:  0.6870201395636366\n",
      "Cost is:  0.6870170449374983\n",
      "Cost is:  0.6870139694939159\n",
      "Cost is:  0.6870109131034899\n",
      "Cost is:  0.6870078756377003\n",
      "Cost is:  0.6870048569689012\n",
      "Cost is:  0.6870018569703138\n",
      "Cost is:  0.6869988755160218\n",
      "Cost is:  0.686995912480964\n",
      "Cost is:  0.6869929677409299\n",
      "Cost is:  0.6869900411725528\n",
      "Cost is:  0.6869871326533041\n",
      "Cost is:  0.6869842420614887\n",
      "Cost is:  0.6869813692762374\n",
      "Cost is:  0.6869785141775032\n",
      "Cost is:  0.6869756766460539\n",
      "Cost is:  0.6869728565634678\n",
      "Cost is:  0.6869700538121278\n",
      "Cost is:  0.6869672682752154\n",
      "Cost is:  0.6869644998367059\n",
      "Cost is:  0.6869617483813625\n",
      "Cost is:  0.6869590137947312\n",
      "Cost is:  0.6869562959631351\n",
      "Cost is:  0.6869535947736697\n",
      "Cost is:  0.6869509101141966\n",
      "Cost is:  0.6869482418733397\n",
      "Cost is:  0.6869455899404779\n",
      "Cost is:  0.6869429542057423\n",
      "Cost is:  0.6869403345600094\n",
      "Cost is:  0.6869377308948965\n",
      "Cost is:  0.6869351431027567\n",
      "Cost is:  0.6869325710766739\n",
      "Cost is:  0.6869300147104576\n",
      "Cost is:  0.6869274738986376\n",
      "Cost is:  0.6869249485364601\n",
      "Cost is:  0.6869224385198821\n",
      "Cost is:  0.6869199437455664\n",
      "Cost is:  0.6869174641108771\n",
      "Cost is:  0.6869149995138744\n",
      "Cost is:  0.6869125498533106\n",
      "Cost is:  0.6869101150286243\n",
      "Cost is:  0.6869076949399372\n",
      "Cost is:  0.6869052894880477\n",
      "Cost is:  0.6869028985744272\n",
      "Cost is:  0.6869005221012157\n",
      "Cost is:  0.6868981599712166\n",
      "Cost is:  0.6868958120878927\n",
      "Cost is:  0.6868934783553614\n",
      "Cost is:  0.6868911586783901\n",
      "Cost is:  0.6868888529623922\n",
      "Cost is:  0.686886561113422\n",
      "Cost is:  0.6868842830381711\n",
      "Cost is:  0.6868820186439639\n",
      "Cost is:  0.6868797678387524\n",
      "Cost is:  0.6868775305311131\n",
      "Cost is:  0.6868753066302415\n",
      "Cost is:  0.6868730960459496\n",
      "Cost is:  0.68687089868866\n",
      "Cost is:  0.6868687144694019\n",
      "Cost is:  0.6868665432998082\n",
      "Cost is:  0.6868643850921106\n",
      "Cost is:  0.6868622397591347\n",
      "Cost is:  0.6868601072142978\n",
      "Cost is:  0.6868579873716028\n",
      "Cost is:  0.6868558801456357\n",
      "Cost is:  0.6868537854515612\n",
      "Cost is:  0.6868517032051183\n",
      "Cost is:  0.6868496333226171\n",
      "Cost is:  0.6868475757209342\n",
      "Cost is:  0.6868455303175096\n",
      "Cost is:  0.6868434970303416\n",
      "Cost is:  0.6868414757779846\n",
      "Cost is:  0.6868394664795442\n",
      "Cost is:  0.6868374690546738\n",
      "Cost is:  0.6868354834235706\n",
      "Cost is:  0.6868335095069721\n",
      "Cost is:  0.6868315472261526\n",
      "Cost is:  0.6868295965029192\n",
      "Cost is:  0.6868276572596083\n",
      "Cost is:  0.6868257294190818\n",
      "Cost is:  0.6868238129047239\n",
      "Cost is:  0.6868219076404374\n",
      "Cost is:  0.6868200135506398\n",
      "Cost is:  0.6868181305602602\n",
      "Cost is:  0.6868162585947356\n",
      "Cost is:  0.6868143975800075\n",
      "Cost is:  0.6868125474425186\n",
      "Cost is:  0.6868107081092093\n",
      "Cost is:  0.6868088795075139\n",
      "Cost is:  0.686807061565358\n",
      "Cost is:  0.6868052542111542\n",
      "Cost is:  0.6868034573737999\n",
      "Cost is:  0.6868016709826729\n",
      "Cost is:  0.6867998949676287\n",
      "Cost is:  0.6867981292589977\n",
      "Cost is:  0.6867963737875807\n",
      "Cost is:  0.6867946284846466\n",
      "Cost is:  0.6867928932819296\n",
      "Cost is:  0.6867911681116244\n",
      "Cost is:  0.686789452906385\n",
      "Cost is:  0.6867877475993204\n",
      "Cost is:  0.6867860521239914\n",
      "Cost is:  0.6867843664144084\n",
      "Cost is:  0.6867826904050277\n",
      "Cost is:  0.6867810240307485\n",
      "Cost is:  0.6867793672269099\n",
      "Cost is:  0.6867777199292883\n",
      "Cost is:  0.6867760820740936\n",
      "Cost is:  0.6867744535979672\n",
      "Cost is:  0.6867728344379783\n",
      "Cost is:  0.6867712245316213\n",
      "Cost is:  0.6867696238168132\n",
      "Cost is:  0.6867680322318898\n",
      "Cost is:  0.6867664497156042\n",
      "Cost is:  0.6867648762071227\n",
      "Cost is:  0.6867633116460224\n",
      "Cost is:  0.6867617559722891\n",
      "Cost is:  0.6867602091263132\n",
      "Cost is:  0.6867586710488881\n",
      "Cost is:  0.6867571416812069\n",
      "Cost is:  0.6867556209648601\n",
      "Cost is:  0.686754108841832\n",
      "Cost is:  0.6867526052544988\n",
      "Cost is:  0.6867511101456258\n",
      "Cost is:  0.6867496234583652\n",
      "Cost is:  0.6867481451362518\n",
      "Cost is:  0.6867466751232028\n",
      "Cost is:  0.686745213363513\n",
      "Cost is:  0.6867437598018539\n",
      "Cost is:  0.6867423143832696\n",
      "Cost is:  0.6867408770531759\n",
      "Cost is:  0.6867394477573565\n",
      "Cost is:  0.686738026441961\n",
      "Cost is:  0.6867366130535026\n",
      "Cost is:  0.6867352075388549\n",
      "Cost is:  0.6867338098452505\n",
      "Cost is:  0.6867324199202773\n",
      "Cost is:  0.6867310377118776\n",
      "Cost is:  0.6867296631683444\n",
      "Cost is:  0.6867282962383192\n",
      "Cost is:  0.6867269368707907\n",
      "Cost is:  0.6867255850150907\n",
      "Cost is:  0.6867242406208935\n",
      "Cost is:  0.6867229036382125\n",
      "Cost is:  0.6867215740173981\n",
      "Cost is:  0.6867202517091356\n",
      "Cost is:  0.6867189366644425\n",
      "Cost is:  0.6867176288346675\n",
      "Cost is:  0.686716328171486\n",
      "Cost is:  0.6867150346268998\n",
      "Cost is:  0.6867137481532346\n",
      "Cost is:  0.6867124687031372\n",
      "Cost is:  0.6867111962295732\n",
      "Cost is:  0.686709930685826\n",
      "Cost is:  0.6867086720254931\n",
      "Cost is:  0.686707420202485\n",
      "Cost is:  0.686706175171023\n",
      "Cost is:  0.6867049368856366\n",
      "Cost is:  0.6867037053011618\n",
      "Cost is:  0.6867024803727391\n",
      "Cost is:  0.686701262055811\n",
      "Cost is:  0.6867000503061201\n",
      "Cost is:  0.6866988450797076\n",
      "Cost is:  0.6866976463329104\n",
      "Cost is:  0.6866964540223598\n",
      "Cost is:  0.6866952681049789\n",
      "Cost is:  0.6866940885379813\n",
      "Cost is:  0.6866929152788684\n",
      "Cost is:  0.6866917482854282\n",
      "Cost is:  0.6866905875157329\n",
      "Cost is:  0.6866894329281368\n",
      "Cost is:  0.6866882844812747\n",
      "Cost is:  0.68668714213406\n",
      "Cost is:  0.6866860058456827\n",
      "Cost is:  0.686684875575608\n",
      "Cost is:  0.686683751283573\n",
      "Cost is:  0.6866826329295865\n",
      "Cost is:  0.6866815204739268\n",
      "Cost is:  0.6866804138771389\n",
      "Cost is:  0.6866793131000337\n",
      "Cost is:  0.6866782181036857\n",
      "Cost is:  0.6866771288494319\n",
      "Cost is:  0.6866760452988686\n",
      "Cost is:  0.6866749674138513\n",
      "Cost is:  0.6866738951564918\n",
      "Cost is:  0.6866728284891568\n",
      "Cost is:  0.6866717673744667\n",
      "Cost is:  0.6866707117752927\n",
      "Cost is:  0.6866696616547565\n",
      "Cost is:  0.6866686169762273\n",
      "Cost is:  0.686667577703321\n",
      "Cost is:  0.686666543799898\n",
      "Cost is:  0.6866655152300623\n",
      "Cost is:  0.6866644919581587\n",
      "Cost is:  0.6866634739487726\n",
      "Cost is:  0.6866624611667265\n",
      "Cost is:  0.6866614535770805\n",
      "Cost is:  0.6866604511451289\n",
      "Cost is:  0.6866594538363999\n",
      "Cost is:  0.6866584616166533\n",
      "Cost is:  0.6866574744518792\n",
      "Cost is:  0.6866564923082961\n",
      "Cost is:  0.6866555151523496\n",
      "Cost is:  0.6866545429507114\n",
      "Cost is:  0.6866535756702767\n",
      "Cost is:  0.6866526132781633\n",
      "Cost is:  0.6866516557417105\n",
      "Cost is:  0.6866507030284765\n",
      "Cost is:  0.6866497551062378\n",
      "Cost is:  0.6866488119429875\n",
      "Cost is:  0.686647873506934\n",
      "Cost is:  0.686646939766499\n",
      "Cost is:  0.6866460106903162\n",
      "Cost is:  0.6866450862472306\n",
      "Cost is:  0.6866441664062963\n",
      "Cost is:  0.6866432511367753\n",
      "Cost is:  0.6866423404081359\n",
      "Cost is:  0.6866414341900522\n",
      "Cost is:  0.6866405324524012\n",
      "Cost is:  0.6866396351652627\n",
      "Cost is:  0.6866387422989173\n",
      "Cost is:  0.6866378538238453\n",
      "Cost is:  0.6866369697107254\n",
      "Cost is:  0.686636089930433\n",
      "Cost is:  0.6866352144540392\n",
      "Cost is:  0.6866343432528094\n",
      "Cost is:  0.6866334762982016\n",
      "Cost is:  0.6866326135618662\n",
      "Cost is:  0.6866317550156432\n",
      "Cost is:  0.6866309006315621\n",
      "Cost is:  0.6866300503818402\n",
      "Cost is:  0.6866292042388811\n",
      "Cost is:  0.6866283621752736\n",
      "Cost is:  0.6866275241637908\n",
      "Cost is:  0.6866266901773885\n",
      "Cost is:  0.686625860189204\n",
      "Cost is:  0.6866250341725548\n",
      "Cost is:  0.6866242121009375\n",
      "Cost is:  0.6866233939480266\n",
      "Cost is:  0.6866225796876734\n",
      "Cost is:  0.6866217692939045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c74e8ae82c26>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# create test/train\n",
    "train_len = round(len(heart_df)*0.8)\n",
    "\n",
    "# define X, y\n",
    "X = np.array(heart_df[['Age','Sex','Chol']])\n",
    "# convert target from yes no to 1 0\n",
    "new_target = pd.Series(heart_df['Target'])\n",
    "y = np.array(new_target.map({'Yes':1,'No':0}))\n",
    "\n",
    "X_train = X[:train_len]\n",
    "y_train = y[:train_len]\n",
    "X_test = X\n",
    "y_test = y\n",
    "\n",
    "\n",
    "# logistic regression\n",
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g\n",
    "\n",
    "def cost_function(X, y, weights):\n",
    "    n, d = X.shape\n",
    "    x_dot_weights = X.dot(weights)\n",
    "    y = y.astype(np.int)\n",
    "    cost = 1.0 / n * (-y.T.dot(np.log(sigmoid(x_dot_weights))) - (1 - y).T.dot(np.log(1 - sigmoid(x_dot_weights))))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient(X, y, weights):\n",
    "    n, d = X.shape\n",
    "    x_dot_weights = X.dot(weights)\n",
    "\n",
    "    grad = (1.0 / n )* (sigmoid(x_dot_weights) - y).T.dot(X)\n",
    "\n",
    "    return grad\n",
    "\n",
    "weights = np.zeros(3)\n",
    "\n",
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 500\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    cost = cost_function(X_train, y_train, weights)\n",
    "    print(\"Cost is: \", cost)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    \n",
    "    # Use the gradient to update the weights\n",
    "    weights = weights - grad*learnin_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - (4 points)\n",
    "\n",
    "Cacluate the accuracy, Precision, Recall and F1 score of your logistic regression implementaion. \n",
    "Print the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.5412541254125413\n",
      "precision is 0.5 for 1\n",
      "recall is 0.16546762589928057 for 1\n",
      "f-measure is 0.24864864864864866 for 1\n",
      "\n",
      "precision is 0.6171617161716172 for 0\n",
      "recall is:  0.8904761904761904 for 0\n",
      "f-measure is 0.7290448343079923 for 0\n"
     ]
    }
   ],
   "source": [
    "# Add your code Here! \n",
    "def predict(weights, X):\n",
    "    p = sigmoid(X.dot(weights)) >= 0.5\n",
    "    return p.astype(int)\n",
    "\n",
    "predictions = predict(weights, X_test)\n",
    "\n",
    "# Correct Predictions are the cases that are equal. \n",
    "\n",
    "correct = np.sum(predictions == y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy is: \", correct/y_test.size)\n",
    "\n",
    "# precision = tp/(tp+fp)\n",
    "# positive = 1\n",
    "# negative = 0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "tn=0\n",
    "for i in range(0,len(predictions)):\n",
    "    if predictions[i] == 1 and predictions[i] == y_test[i]:\n",
    "        tp += 1\n",
    "    if predictions[i] == 1 and predictions[i] != y_test[i]:\n",
    "        fp += 1\n",
    "    if predictions[i] == 0 and predictions[i] != y_test[i]:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "precision = tp/(tp+fp)\n",
    "print('precision is',precision,'for 1')\n",
    "recall = tp/(tp+fn)\n",
    "print('recall is',recall,'for 1')\n",
    "# f-measure\n",
    "f_measure = 2*precision*recall/(precision+recall)\n",
    "print('f-measure is',f_measure,'for 1')\n",
    "print()\n",
    "precision2 = tn / (tn + fn)\n",
    "recall2 = tn / (tn + fp)\n",
    "f_2 = 2 * precision2 * recall2 / (precision2 + recall2)\n",
    "print('precision is', precision2,'for 0')\n",
    "print('recall is: ', recall2,'for 0')\n",
    "print('f-measure is', f_2,'for 0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - (4 points)\n",
    "\n",
    "\n",
    "Add y-intercept and repeat the above 2 tasks. Do you see any differences after adding the y-intercept? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c74e8ae82c26>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is:  0.6931471805599453\n",
      "Cost is:  0.6911185326894426\n",
      "Cost is:  0.6900363280289841\n",
      "Cost is:  0.689534584014115\n",
      "Cost is:  0.6892691675169696\n",
      "Cost is:  0.6891345600038361\n",
      "Cost is:  0.6890588425071298\n",
      "Cost is:  0.689013940697307\n",
      "Cost is:  0.688983615036766\n",
      "Cost is:  0.6889607322474902\n",
      "Cost is:  0.6889414897981837\n",
      "Cost is:  0.6889241176997609\n",
      "Cost is:  0.688907709665768\n",
      "Cost is:  0.688891831418801\n",
      "Cost is:  0.6888762647688124\n",
      "Cost is:  0.6888609034225133\n",
      "Cost is:  0.6888456942513534\n",
      "Cost is:  0.6888306109284995\n",
      "Cost is:  0.6888156401051904\n",
      "Cost is:  0.6888007748851921\n",
      "Cost is:  0.6887860115311927\n",
      "Cost is:  0.6887713478645602\n",
      "Cost is:  0.6887567824771754\n",
      "Cost is:  0.6887423143399052\n",
      "Cost is:  0.6887279426142406\n",
      "Cost is:  0.6887136665561677\n",
      "Cost is:  0.6886994854714905\n",
      "Cost is:  0.6886853986919635\n",
      "Cost is:  0.6886714055648773\n",
      "Cost is:  0.6886575054469843\n",
      "Cost is:  0.6886436977021564\n",
      "Cost is:  0.6886299816997572\n",
      "Cost is:  0.6886163568141511\n",
      "Cost is:  0.688602822424221\n",
      "Cost is:  0.6885893779132732\n",
      "Cost is:  0.6885760226688697\n",
      "Cost is:  0.6885627560828039\n",
      "Cost is:  0.6885495775510247\n",
      "Cost is:  0.68853648647362\n",
      "Cost is:  0.6885234822547691\n",
      "Cost is:  0.6885105643027246\n",
      "Cost is:  0.688497732029775\n",
      "Cost is:  0.6884849848522234\n",
      "Cost is:  0.6884723221903557\n",
      "Cost is:  0.6884597434684163\n",
      "Cost is:  0.6884472481145798\n",
      "Cost is:  0.6884348355609248\n",
      "Cost is:  0.6884225052434085\n",
      "Cost is:  0.6884102566018387\n",
      "Cost is:  0.6883980890798491\n",
      "Cost is:  0.688386002124873\n",
      "Cost is:  0.6883739951881171\n",
      "Cost is:  0.6883620677245373\n",
      "Cost is:  0.6883502191928116\n",
      "Cost is:  0.688338449055316\n",
      "Cost is:  0.6883267567780996\n",
      "Cost is:  0.6883151418308588\n",
      "Cost is:  0.688303603686913\n",
      "Cost is:  0.6882921418231808\n",
      "Cost is:  0.6882807557201543\n",
      "Cost is:  0.6882694448618757\n",
      "Cost is:  0.6882582087359127\n",
      "Cost is:  0.6882470468333357\n",
      "Cost is:  0.6882359586486921\n",
      "Cost is:  0.6882249436799847\n",
      "Cost is:  0.6882140014286465\n",
      "Cost is:  0.6882031313995189\n",
      "Cost is:  0.688192333100828\n",
      "Cost is:  0.6881816060441607\n",
      "Cost is:  0.688170949744443\n",
      "Cost is:  0.6881603637199172\n",
      "Cost is:  0.6881498474921185\n",
      "Cost is:  0.6881394005858538\n",
      "Cost is:  0.6881290225291785\n",
      "Cost is:  0.6881187128533746\n",
      "Cost is:  0.6881084710929292\n",
      "Cost is:  0.6880982967855119\n",
      "Cost is:  0.6880881894719542\n",
      "Cost is:  0.6880781486962264\n",
      "Cost is:  0.6880681740054182\n",
      "Cost is:  0.6880582649497157\n",
      "Cost is:  0.6880484210823815\n",
      "Cost is:  0.688038641959733\n",
      "Cost is:  0.6880289271411215\n",
      "Cost is:  0.6880192761889127\n",
      "Cost is:  0.6880096886684642\n",
      "Cost is:  0.6880001641481074\n",
      "Cost is:  0.687990702199125\n",
      "Cost is:  0.6879813023957324\n",
      "Cost is:  0.687971964315057\n",
      "Cost is:  0.6879626875371185\n",
      "Cost is:  0.6879534716448094\n",
      "Cost is:  0.6879443162238746\n",
      "Cost is:  0.6879352208628932\n",
      "Cost is:  0.6879261851532573\n",
      "Cost is:  0.6879172086891556\n",
      "Cost is:  0.6879082910675509\n",
      "Cost is:  0.6878994318881637\n",
      "Cost is:  0.6878906307534527\n",
      "Cost is:  0.687881887268595\n",
      "Cost is:  0.6878732010414697\n",
      "Cost is:  0.6878645716826369\n",
      "Cost is:  0.6878559988053216\n",
      "Cost is:  0.6878474820253936\n",
      "Cost is:  0.6878390209613513\n",
      "Cost is:  0.6878306152343018\n",
      "Cost is:  0.6878222644679449\n",
      "Cost is:  0.6878139682885537\n",
      "Cost is:  0.6878057263249583\n",
      "Cost is:  0.6877975382085274\n",
      "Cost is:  0.6877894035731514\n",
      "Cost is:  0.6877813220552252\n",
      "Cost is:  0.6877732932936307\n",
      "Cost is:  0.6877653169297202\n",
      "Cost is:  0.6877573926072988\n",
      "Cost is:  0.6877495199726089\n",
      "Cost is:  0.6877416986743116\n",
      "Cost is:  0.687733928363472\n",
      "Cost is:  0.6877262086935417\n",
      "Cost is:  0.6877185393203429\n",
      "Cost is:  0.6877109199020515\n",
      "Cost is:  0.6877033500991818\n",
      "Cost is:  0.6876958295745703\n",
      "Cost is:  0.6876883579933589\n",
      "Cost is:  0.6876809350229806\n",
      "Cost is:  0.6876735603331425\n",
      "Cost is:  0.687666233595811\n",
      "Cost is:  0.687658954485196\n",
      "Cost is:  0.6876517226777353\n",
      "Cost is:  0.6876445378520798\n",
      "Cost is:  0.6876373996890782\n",
      "Cost is:  0.6876303078717616\n",
      "Cost is:  0.687623262085329\n",
      "Cost is:  0.687616262017132\n",
      "Cost is:  0.6876093073566606\n",
      "Cost is:  0.6876023977955276\n",
      "Cost is:  0.6875955330274552\n",
      "Cost is:  0.6875887127482596\n",
      "Cost is:  0.6875819366558373\n",
      "Cost is:  0.6875752044501501\n",
      "Cost is:  0.6875685158332117\n",
      "Cost is:  0.6875618705090729\n",
      "Cost is:  0.6875552681838087\n",
      "Cost is:  0.6875487085655028\n",
      "Cost is:  0.6875421913642354\n",
      "Cost is:  0.6875357162920684\n",
      "Cost is:  0.6875292830630325\n",
      "Cost is:  0.6875228913931134\n",
      "Cost is:  0.6875165410002381\n",
      "Cost is:  0.6875102316042618\n",
      "Cost is:  0.6875039629269555\n",
      "Cost is:  0.6874977346919913\n",
      "Cost is:  0.6874915466249301\n",
      "Cost is:  0.6874853984532092\n",
      "Cost is:  0.6874792899061284\n",
      "Cost is:  0.687473220714838\n",
      "Cost is:  0.6874671906123258\n",
      "Cost is:  0.6874611993334039\n",
      "Cost is:  0.6874552466146974\n",
      "Cost is:  0.6874493321946309\n",
      "Cost is:  0.6874434558134167\n",
      "Cost is:  0.6874376172130424\n",
      "Cost is:  0.6874318161372585\n",
      "Cost is:  0.6874260523315664\n",
      "Cost is:  0.6874203255432066\n",
      "Cost is:  0.6874146355211468\n",
      "Cost is:  0.6874089820160691\n",
      "Cost is:  0.6874033647803597\n",
      "Cost is:  0.687397783568096\n",
      "Cost is:  0.6873922381350357\n",
      "Cost is:  0.6873867282386047\n",
      "Cost is:  0.687381253637886\n",
      "Cost is:  0.6873758140936085\n",
      "Cost is:  0.6873704093681352\n",
      "Cost is:  0.6873650392254521\n",
      "Cost is:  0.6873597034311575\n",
      "Cost is:  0.6873544017524502\n",
      "Cost is:  0.6873491339581191\n",
      "Cost is:  0.6873438998185324\n",
      "Cost is:  0.6873386991056255\n",
      "Cost is:  0.6873335315928921\n",
      "Cost is:  0.6873283970553722\n",
      "Cost is:  0.6873232952696414\n",
      "Cost is:  0.6873182260138014\n",
      "Cost is:  0.6873131890674689\n",
      "Cost is:  0.6873081842117644\n",
      "Cost is:  0.6873032112293034\n",
      "Cost is:  0.6872982699041851\n",
      "Cost is:  0.6872933600219826\n",
      "Cost is:  0.6872884813697324\n",
      "Cost is:  0.687283633735925\n",
      "Cost is:  0.6872788169104941\n",
      "Cost is:  0.6872740306848074\n",
      "Cost is:  0.6872692748516561\n",
      "Cost is:  0.6872645492052459\n",
      "Cost is:  0.6872598535411868\n",
      "Cost is:  0.6872551876564831\n",
      "Cost is:  0.6872505513495245\n",
      "Cost is:  0.6872459444200762\n",
      "Cost is:  0.6872413666692698\n",
      "Cost is:  0.6872368178995935\n",
      "Cost is:  0.6872322979148827\n",
      "Cost is:  0.6872278065203115\n",
      "Cost is:  0.6872233435223822\n",
      "Cost is:  0.6872189087289183\n",
      "Cost is:  0.6872145019490526\n",
      "Cost is:  0.6872101229932209\n",
      "Cost is:  0.6872057716731509\n",
      "Cost is:  0.6872014478018551\n",
      "Cost is:  0.6871971511936203\n",
      "Cost is:  0.6871928816640005\n",
      "Cost is:  0.6871886390298069\n",
      "Cost is:  0.6871844231091001\n",
      "Cost is:  0.6871802337211808\n",
      "Cost is:  0.6871760706865818\n",
      "Cost is:  0.6871719338270598\n",
      "Cost is:  0.6871678229655857\n",
      "Cost is:  0.6871637379263379\n",
      "Cost is:  0.6871596785346931\n",
      "Cost is:  0.687155644617218\n",
      "Cost is:  0.6871516360016616\n",
      "Cost is:  0.6871476525169469\n",
      "Cost is:  0.6871436939931621\n",
      "Cost is:  0.6871397602615542\n",
      "Cost is:  0.6871358511545197\n",
      "Cost is:  0.6871319665055968\n",
      "Cost is:  0.6871281061494584\n",
      "Cost is:  0.6871242699219036\n",
      "Cost is:  0.6871204576598501\n",
      "Cost is:  0.6871166692013264\n",
      "Cost is:  0.6871129043854649\n",
      "Cost is:  0.687109163052493\n",
      "Cost is:  0.6871054450437273\n",
      "Cost is:  0.6871017502015638\n",
      "Cost is:  0.6870980783694738\n",
      "Cost is:  0.6870944293919927\n",
      "Cost is:  0.6870908031147156\n",
      "Cost is:  0.6870871993842887\n",
      "Cost is:  0.6870836180484025\n",
      "Cost is:  0.6870800589557844\n",
      "Cost is:  0.6870765219561918\n",
      "Cost is:  0.6870730069004051\n",
      "Cost is:  0.6870695136402202\n",
      "Cost is:  0.6870660420284417\n",
      "Cost is:  0.6870625919188768\n",
      "Cost is:  0.687059163166327\n",
      "Cost is:  0.6870557556265824\n",
      "Cost is:  0.6870523691564148\n",
      "Cost is:  0.6870490036135701\n",
      "Cost is:  0.6870456588567623\n",
      "Cost is:  0.6870423347456673\n",
      "Cost is:  0.6870390311409155\n",
      "Cost is:  0.6870357479040848\n",
      "Cost is:  0.6870324848976963\n",
      "Cost is:  0.6870292419852051\n",
      "Cost is:  0.6870260190309958\n",
      "Cost is:  0.6870228159003748\n",
      "Cost is:  0.6870196324595658\n",
      "Cost is:  0.6870164685757012\n",
      "Cost is:  0.6870133241168174\n",
      "Cost is:  0.6870101989518486\n",
      "Cost is:  0.6870070929506199\n",
      "Cost is:  0.6870040059838415\n",
      "Cost is:  0.687000937923103\n",
      "Cost is:  0.6869978886408669\n",
      "Cost is:  0.6869948580104626\n",
      "Cost is:  0.6869918459060809\n",
      "Cost is:  0.6869888522027678\n",
      "Cost is:  0.6869858767764185\n",
      "Cost is:  0.6869829195037718\n",
      "Cost is:  0.6869799802624043\n",
      "Cost is:  0.6869770589307246\n",
      "Cost is:  0.6869741553879676\n",
      "Cost is:  0.686971269514189\n",
      "Cost is:  0.6869684011902591\n",
      "Cost is:  0.6869655502978581\n",
      "Cost is:  0.6869627167194701\n",
      "Cost is:  0.6869599003383771\n",
      "Cost is:  0.6869571010386544\n",
      "Cost is:  0.6869543187051649\n",
      "Cost is:  0.6869515532235531\n",
      "Cost is:  0.6869488044802404\n",
      "Cost is:  0.6869460723624201\n",
      "Cost is:  0.6869433567580512\n",
      "Cost is:  0.6869406575558531\n",
      "Cost is:  0.686937974645302\n",
      "Cost is:  0.6869353079166236\n",
      "Cost is:  0.6869326572607894\n",
      "Cost is:  0.6869300225695112\n",
      "Cost is:  0.6869274037352355\n",
      "Cost is:  0.6869248006511393\n",
      "Cost is:  0.6869222132111248\n",
      "Cost is:  0.6869196413098142\n",
      "Cost is:  0.6869170848425451\n",
      "Cost is:  0.6869145437053652\n",
      "Cost is:  0.686912017795028\n",
      "Cost is:  0.6869095070089873\n",
      "Cost is:  0.686907011245393\n",
      "Cost is:  0.6869045304030862\n",
      "Cost is:  0.686902064381594\n",
      "Cost is:  0.6868996130811252\n",
      "Cost is:  0.6868971764025662\n",
      "Cost is:  0.686894754247475\n",
      "Cost is:  0.686892346518078\n",
      "Cost is:  0.686889953117264\n",
      "Cost is:  0.6868875739485814\n",
      "Cost is:  0.6868852089162322\n",
      "Cost is:  0.686882857925068\n",
      "Cost is:  0.6868805208805859\n",
      "Cost is:  0.6868781976889241\n",
      "Cost is:  0.6868758882568565\n",
      "Cost is:  0.6868735924917896\n",
      "Cost is:  0.6868713103017575\n",
      "Cost is:  0.6868690415954176\n",
      "Cost is:  0.6868667862820471\n",
      "Cost is:  0.6868645442715376\n",
      "Cost is:  0.6868623154743917\n",
      "Cost is:  0.6868600998017184\n",
      "Cost is:  0.6868578971652293\n",
      "Cost is:  0.6868557074772343\n",
      "Cost is:  0.6868535306506376\n",
      "Cost is:  0.6868513665989336\n",
      "Cost is:  0.6868492152362027\n",
      "Cost is:  0.6868470764771072\n",
      "Cost is:  0.6868449502368882\n",
      "Cost is:  0.6868428364313603\n",
      "Cost is:  0.6868407349769092\n",
      "Cost is:  0.686838645790486\n",
      "Cost is:  0.6868365687896052\n",
      "Cost is:  0.6868345038923394\n",
      "Cost is:  0.6868324510173162\n",
      "Cost is:  0.6868304100837145\n",
      "Cost is:  0.68682838101126\n",
      "Cost is:  0.6868263637202225\n",
      "Cost is:  0.6868243581314114\n",
      "Cost is:  0.6868223641661722\n",
      "Cost is:  0.6868203817463829\n",
      "Cost is:  0.6868184107944507\n",
      "Cost is:  0.6868164512333077\n",
      "Cost is:  0.6868145029864077\n",
      "Cost is:  0.6868125659777224\n",
      "Cost is:  0.6868106401317389\n",
      "Cost is:  0.6868087253734544\n",
      "Cost is:  0.686806821628374\n",
      "Cost is:  0.6868049288225072\n",
      "Cost is:  0.6868030468823637\n",
      "Cost is:  0.686801175734951\n",
      "Cost is:  0.6867993153077697\n",
      "Cost is:  0.6867974655288114\n",
      "Cost is:  0.6867956263265549\n",
      "Cost is:  0.6867937976299625\n",
      "Cost is:  0.6867919793684772\n",
      "Cost is:  0.6867901714720193\n",
      "Cost is:  0.6867883738709826\n",
      "Cost is:  0.6867865864962325\n",
      "Cost is:  0.686784809279101\n",
      "Cost is:  0.6867830421513852\n",
      "Cost is:  0.6867812850453427\n",
      "Cost is:  0.6867795378936898\n",
      "Cost is:  0.6867778006295973\n",
      "Cost is:  0.6867760731866879\n",
      "Cost is:  0.6867743554990331\n",
      "Cost is:  0.6867726475011497\n",
      "Cost is:  0.686770949127998\n",
      "Cost is:  0.6867692603149768\n",
      "Cost is:  0.6867675809979223\n",
      "Cost is:  0.686765911113104\n",
      "Cost is:  0.6867642505972225\n",
      "Cost is:  0.6867625993874052\n",
      "Cost is:  0.6867609574212056\n",
      "Cost is:  0.6867593246365981\n",
      "Cost is:  0.6867577009719767\n",
      "Cost is:  0.6867560863661516\n",
      "Cost is:  0.6867544807583461\n",
      "Cost is:  0.6867528840881942\n",
      "Cost is:  0.6867512962957377\n",
      "Cost is:  0.6867497173214234\n",
      "Cost is:  0.6867481471061004\n",
      "Cost is:  0.686746585591017\n",
      "Cost is:  0.6867450327178184\n",
      "Cost is:  0.6867434884285438\n",
      "Cost is:  0.6867419526656243\n",
      "Cost is:  0.6867404253718787\n",
      "Cost is:  0.6867389064905132\n",
      "Cost is:  0.6867373959651161\n",
      "Cost is:  0.6867358937396573\n",
      "Cost is:  0.6867343997584848\n",
      "Cost is:  0.6867329139663222\n",
      "Cost is:  0.6867314363082665\n",
      "Cost is:  0.6867299667297847\n",
      "Cost is:  0.6867285051767122\n",
      "Cost is:  0.6867270515952499\n",
      "Cost is:  0.6867256059319622\n",
      "Cost is:  0.686724168133773\n",
      "Cost is:  0.6867227381479653\n",
      "Cost is:  0.6867213159221776\n",
      "Cost is:  0.6867199014044011\n",
      "Cost is:  0.6867184945429787\n",
      "Cost is:  0.6867170952866015\n",
      "Cost is:  0.6867157035843063\n",
      "Cost is:  0.6867143193854742\n",
      "Cost is:  0.6867129426398273\n",
      "Cost is:  0.6867115732974273\n",
      "Cost is:  0.6867102113086723\n",
      "Cost is:  0.6867088566242949\n",
      "Cost is:  0.6867075091953603\n",
      "Cost is:  0.6867061689732631\n",
      "Cost is:  0.6867048359097263\n",
      "Cost is:  0.6867035099567979\n",
      "Cost is:  0.6867021910668493\n",
      "Cost is:  0.6867008791925733\n",
      "Cost is:  0.6866995742869813\n",
      "Cost is:  0.6866982763034013\n",
      "Cost is:  0.6866969851954764\n",
      "Cost is:  0.6866957009171618\n",
      "Cost is:  0.6866944234227229\n",
      "Cost is:  0.6866931526667339\n",
      "Cost is:  0.6866918886040745\n",
      "Cost is:  0.6866906311899287\n",
      "Cost is:  0.6866893803797828\n",
      "Cost is:  0.6866881361294224\n",
      "Cost is:  0.6866868983949315\n",
      "Cost is:  0.6866856671326897\n",
      "Cost is:  0.6866844422993709\n",
      "Cost is:  0.6866832238519404\n",
      "Cost is:  0.6866820117476533\n",
      "Cost is:  0.6866808059440533\n",
      "Cost is:  0.6866796063989694\n",
      "Cost is:  0.686678413070515\n",
      "Cost is:  0.6866772259170854\n",
      "Cost is:  0.686676044897356\n",
      "Cost is:  0.6866748699702809\n",
      "Cost is:  0.6866737010950904\n",
      "Cost is:  0.686672538231289\n",
      "Cost is:  0.6866713813386544\n",
      "Cost is:  0.6866702303772344\n",
      "Cost is:  0.6866690853073466\n",
      "Cost is:  0.6866679460895749\n",
      "Cost is:  0.6866668126847697\n",
      "Cost is:  0.6866656850540435\n",
      "Cost is:  0.6866645631587714\n",
      "Cost is:  0.6866634469605888\n",
      "Cost is:  0.6866623364213882\n",
      "Cost is:  0.6866612315033194\n",
      "Cost is:  0.6866601321687867\n",
      "Cost is:  0.6866590383804476\n",
      "Cost is:  0.6866579501012103\n",
      "Cost is:  0.6866568672942334\n",
      "Cost is:  0.6866557899229225\n",
      "Cost is:  0.6866547179509298\n",
      "Cost is:  0.6866536513421525\n",
      "Cost is:  0.6866525900607297\n",
      "Cost is:  0.6866515340710424\n",
      "Cost is:  0.6866504833377113\n",
      "Cost is:  0.6866494378255946\n",
      "Cost is:  0.686648397499787\n",
      "Cost is:  0.6866473623256183\n",
      "Cost is:  0.6866463322686507\n",
      "Cost is:  0.6866453072946788\n",
      "Cost is:  0.6866442873697268\n",
      "Cost is:  0.6866432724600475\n",
      "Cost is:  0.6866422625321206\n",
      "Cost is:  0.6866412575526509\n",
      "Cost is:  0.6866402574885675\n",
      "Cost is:  0.6866392623070215\n",
      "Cost is:  0.6866382719753847\n",
      "Cost is:  0.686637286461249\n",
      "Cost is:  0.6866363057324232\n",
      "Cost is:  0.6866353297569329\n",
      "Cost is:  0.6866343585030186\n",
      "Cost is:  0.6866333919391343\n",
      "Cost is:  0.6866324300339459\n",
      "Cost is:  0.6866314727563299\n",
      "Cost is:  0.6866305200753717\n",
      "Cost is:  0.6866295719603651\n",
      "Cost is:  0.6866286283808095\n",
      "Cost is:  0.6866276893064097\n",
      "Cost is:  0.6866267547070738\n",
      "Cost is:  0.6866258245529122\n",
      "Cost is:  0.6866248988142358\n",
      "Cost is:  0.6866239774615555\n",
      "Cost is:  0.6866230604655796\n",
      "Cost is:  0.6866221477972139\n",
      "Cost is:  0.6866212394275588\n",
      "Cost is:  0.6866203353279096\n",
      "Cost is:  0.6866194354697536\n",
      "Cost is:  0.6866185398247703\n",
      "Cost is:  0.6866176483648289\n",
      "Cost is:  0.6866167610619879\n",
      "Cost is:  0.6866158778884928\n",
      "Cost is:  0.6866149988167762\n",
      "Cost is:  0.686614123819455\n",
      "Cost is:  0.6866132528693308\n",
      "Cost is:  0.6866123859393872\n",
      "Cost is:  0.6866115230027893\n",
      "Cost is:  0.6866106640328823\n",
      "Cost is:  0.6866098090031905\n",
      "Cost is:  0.6866089578874158\n",
      "Cost is:  0.6866081106594366\n",
      "Cost is:  0.6866072672933066\n"
     ]
    }
   ],
   "source": [
    "# add y-intercept\n",
    "intercept = np.ones(len(heart_df))\n",
    "heart_df['yInt']=intercept\n",
    "\n",
    "# create test/train\n",
    "train_len = round(len(heart_df)*0.8)\n",
    "\n",
    "# define X, y\n",
    "X = np.array(heart_df[['Age','Sex','Chol','yInt']])\n",
    "# convert target from yes no to 1 0\n",
    "new_target = pd.Series(heart_df['Target'])\n",
    "y = np.array(new_target.map({'Yes':1,'No':0}))\n",
    "\n",
    "X_train = X[:train_len]\n",
    "y_train = y[:train_len]\n",
    "X_test = X\n",
    "y_test = y\n",
    "\n",
    "weights = np.zeros(4)\n",
    "\n",
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 500\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    cost = cost_function(X_train, y_train, weights)\n",
    "    print(\"Cost is: \", cost)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    \n",
    "    # Use the gradient to update the weights\n",
    "    weights = weights - grad*learnin_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.5412541254125413\n",
      "precision is 0.5 for 1\n",
      "recall is 0.02877697841726619 for 1\n",
      "f-measure is 0.05442176870748299 for 1\n",
      "\n",
      "precision is 0.5544554455445545 for 0\n",
      "recall is:  0.9767441860465116 for 0\n",
      "f-measure is 0.7073684210526316 for 0\n",
      "\n",
      "There is a difference.\n",
      "After adding the y-intercept, the accuracy, precision and f-feature slightly increases for 1.\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(weights, X_test)\n",
    "\n",
    "# Correct Predictions are the cases that are equal. \n",
    "\n",
    "correct = np.sum(predictions == y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy is: \", correct/y_test.size)\n",
    "\n",
    "# precision = tp/(tp+fp)\n",
    "# positive = 1\n",
    "# negative = 0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "tn=0\n",
    "for i in range(0,len(predictions)):\n",
    "    if predictions[i] == 1 and predictions[i] == y_test[i]:\n",
    "        tp += 1\n",
    "    if predictions[i] == 1 and predictions[i] != y_test[i]:\n",
    "        fp += 1\n",
    "    if predictions[i] == 0 and predictions[i] != y_test[i]:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "precision = tp/(tp+fp)\n",
    "print('precision is',precision,'for 1')\n",
    "recall = tp/(tp+fn)\n",
    "print('recall is',recall,'for 1')\n",
    "# f-measure\n",
    "f_measure = 2*precision*recall/(precision+recall)\n",
    "print('f-measure is',f_measure,'for 1')\n",
    "print()\n",
    "precision2 = tn / (tn + fn)\n",
    "recall2 = tn / (tn + fp)\n",
    "f_2 = 2 * precision2 * recall2 / (precision2 + recall2)\n",
    "print('precision is', precision2,'for 0')\n",
    "print('recall is: ', recall2,'for 0')\n",
    "print('f-measure is', f_2,'for 0')\n",
    "print()\n",
    "print('There is a difference.')\n",
    "print('After adding the y-intercept, the accuracy, precision and f-feature slightly increases for 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Implement the Bold Driver   - (4 points)\n",
    "In your GD implementation, add the bold driver idea to have a dynamic learning rate.\n",
    "\n",
    "* Add a stop codition to stop the GD when the cost is not changing more than 0.001. (differences between two costs not more than 0.001, then stop)\n",
    "* Can you stop earlier than 100 iterations? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is:  0.6874507744351089\n",
      "Cost is:  0.6874448933366729\n",
      "Iteration stopped at 1 earlier than 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c74e8ae82c26>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 500\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "# Implementation here is removed. \n",
    "\n",
    "# Your task to implement the GD here. \n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    cost = cost_function(X_train, y_train, weights)\n",
    "    print(\"Cost is: \", cost)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    weights = weights - grad*learnin_rate\n",
    "    \n",
    "    precision = 0.001\n",
    "    # bold driver\n",
    "    if i > 0 and np.abs(cost_list[i-1]-cost_list[i]) < precision:\n",
    "        print('Iteration stopped at',i,'earlier than 100 iterations')\n",
    "        break\n",
    "    if i > 0 and cost_list[i-1] > cost_list[i]:\n",
    "        learnin_rate = 1.05*learnin_rate\n",
    "    if i > 0 and cost_list[i-1] < cost_list[i]:\n",
    "        learnin_rate = 0.5 * learnin_rate\n",
    "\n",
    "    \n",
    "    old_cost = cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Implement the L2 norm regularization.  - (4 points)\n",
    "\n",
    "Modify your Cost and gradient to implement the l2 norm regularization. \n",
    "Repreat the task 1 and 2 to check if your result is changing. \n",
    "\n",
    "* Use y-itercept. \n",
    "* Do max 100 iterations as before and report your accuracy, Precision, Recall and F1. \n",
    "* You can stop earlier when the cost is not changing than 0.001. \n",
    "\n",
    "\n",
    "**Optional:** you might want to use the bold driver. But you can do this task without the bold driver as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is 167.74161769550676\n",
      "Cost is 167.25068630076058\n",
      "Cost is 166.9890055644544\n",
      "Cost is 166.89713551585277\n",
      "Cost is 166.85716117032223\n",
      "Cost is 166.84669533898503\n",
      "Cost is 166.85582969822406\n",
      "Cost is 166.89634329265928\n",
      "Cost is 166.730032672741\n",
      "Cost is 166.72794686667228\n",
      "Cost is 166.72647725626302\n",
      "Cost is 166.72509649336138\n",
      "Cost is 166.72368136824687\n",
      "Cost is 166.72220460499284\n",
      "Cost is 166.72065892004343\n",
      "Cost is 166.71904057631292\n",
      "Cost is 166.71734629563855\n",
      "Cost is 166.7155727616543\n",
      "Cost is 166.713716550295\n",
      "Cost is 166.7117741201366\n",
      "Cost is 166.70974181029757\n",
      "Cost is 166.70761583962397\n",
      "Cost is 166.70539230646577\n",
      "Cost is 166.70306718898595\n",
      "Cost is 166.70063634604023\n",
      "Cost is 166.69809551869\n",
      "Cost is 166.69544033242389\n",
      "Cost is 166.69266630016799\n",
      "Cost is 166.68976882617122\n",
      "Cost is 166.68674321085933\n",
      "Cost is 166.6835846567562\n",
      "Cost is 166.68028827557708\n",
      "Cost is 166.6768490966064\n",
      "Cost is 166.6732620764766\n",
      "Cost is 166.6695221104712\n",
      "Cost is 166.66562404548105\n",
      "Cost is 166.66156269474607\n",
      "Cost is 166.65733285452086\n",
      "Cost is 166.65292932280346\n",
      "Cost is 166.64834692026997\n",
      "Cost is 166.64358051355697\n",
      "Cost is 166.63862504103352\n",
      "Cost is 166.63347554119977\n",
      "Cost is 166.62812718384347\n",
      "Cost is 166.62257530407703\n",
      "Cost is 166.6168154393648\n",
      "Cost is 166.6108433696338\n",
      "Cost is 166.60465516054057\n",
      "Cost is 166.59824720994078\n",
      "Cost is 166.5916162975762\n",
      "Cost is 166.58475963795635\n",
      "Cost is 166.57767493636692\n",
      "Cost is 166.57036044788566\n",
      "Cost is 166.56281503922526\n",
      "Cost is 166.5550382531536\n",
      "Cost is 166.54703037516882\n",
      "Cost is 166.53879250200174\n",
      "Cost is 166.53032661149246\n",
      "Cost is 166.52163563295593\n",
      "Cost is 166.51272351870463\n",
      "Cost is 166.5035953081951\n",
      "Cost is 166.49425723023913\n",
      "Cost is 166.48471660870828\n",
      "Cost is 166.47498550606048\n",
      "Cost is 166.46517711877715\n",
      "Cost is 166.4613474508976\n",
      "Cost is 166.8250926603381\n",
      "Cost is 190.348396840357\n",
      "Cost is 362.83656910450645\n",
      "Cost is 225.43342566279506\n",
      "Cost is 167.18612106633387\n",
      "Cost is 166.45175722154116\n",
      "Cost is 166.4246034647186\n",
      "Cost is 166.42139143340364\n",
      "Cost is 166.4197595905094\n",
      "Cost is 166.4181997396719\n",
      "Cost is 166.416601255417\n",
      "Cost is 166.41493293832906\n",
      "Cost is 166.41319562082776\n",
      "Cost is 166.4113821545718\n",
      "Cost is 166.409492633207\n",
      "Cost is 166.4075220628114\n",
      "Cost is 166.4054699208117\n",
      "Cost is 166.40333139931104\n",
      "Cost is 166.40110633809826\n",
      "Cost is 166.39878917103673\n",
      "Cost is 166.39638181631048\n",
      "Cost is 166.39387674882317\n",
      "Cost is 166.39128633756567\n",
      "Cost is 166.3886105684338\n",
      "Cost is 166.38597805304528\n",
      "Cost is 166.3838364568533\n",
      "Cost is 166.3855086669864\n",
      "Cost is 166.41236653820778\n",
      "Cost is 166.39612712628875\n",
      "Cost is 166.37469246452574\n",
      "Cost is 166.3735722389159\n",
      "Cost is 166.37272775737983\n",
      "Iteration stopped at 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c74e8ae82c26>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# Add your code Here! \n",
    "num_iterations = 100\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "weights = np.zeros(4)\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "lamb = 1\n",
    "def sigmoid(v):\n",
    "    return 1/(1+np.e**-v)\n",
    "\n",
    "def logistic_regression(X, weights):\n",
    "    pred = np.sum(weights*X)\n",
    "    return sigmoid(pred)\n",
    "\n",
    "def mse(y, pred):\n",
    "    return (-1/len(heart_df)) * sum((y-pred)**2)\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # predictions\n",
    "    # pred = logistic_regression(X_train, weights)\n",
    "    \n",
    "    # calculate MSE\n",
    "    # cost = mse(y_train, pred)*len(y_train)\n",
    "    \n",
    "    # calculate gradient\n",
    "    cost = cost_function(X_train, y_train, weights)*len(y_train)\n",
    "    \n",
    "    # calculate ridge regression/penalty\n",
    "    ridge_cost = lamb*sum([w**2 for w in weights[:-1]])\n",
    "    cost += ridge_cost\n",
    "    print('Cost is', cost)\n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    ridge_gradient = lamb * sum(weights[:-1])\n",
    "    weights = weights - learnin_rate * (grad + ridge_gradient)\n",
    "    \n",
    "    precision = 0.001\n",
    "    # bold driver\n",
    "    if i > 0 and np.abs(cost_list[i-1]-cost_list[i]) < precision:\n",
    "        print('Iteration stopped at',i)\n",
    "        break\n",
    "    if i > 0 and cost_list[i-1] > cost_list[i]:\n",
    "        learnin_rate = 1.05*learnin_rate\n",
    "    if i > 0 and cost_list[i-1] < cost_list[i]:\n",
    "        learnin_rate = 0.5 * learnin_rate\n",
    "\n",
    "    \n",
    "    old_cost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.5412541254125413\n",
      "precision is 0.5 for 1\n",
      "recall is 0.02877697841726619 for 1\n",
      "f-measure is 0.05442176870748299 for 1\n",
      "\n",
      "precision is 0.5544554455445545 for 0\n",
      "recall is:  0.9767441860465116 for 0\n",
      "f-measure is 0.7073684210526316 for 0\n",
      "\n",
      "After implemented L2 norm regularization, I did not see any change with the accuracy, precision, recall or f-measure.\n",
      "They are the same as those of logistic regression with y-intercept and bold driver. \n"
     ]
    }
   ],
   "source": [
    "predictions = predict(weights, X_test)\n",
    "\n",
    "# Correct Predictions are the cases that are equal. \n",
    "\n",
    "correct = np.sum(predictions == y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy is: \", correct/y_test.size)\n",
    "\n",
    "# precision = tp/(tp+fp)\n",
    "# positive = 1\n",
    "# negative = 0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "tn=0\n",
    "for i in range(0,len(predictions)):\n",
    "    if predictions[i] == 1 and predictions[i] == y_test[i]:\n",
    "        tp += 1\n",
    "    if predictions[i] == 1 and predictions[i] != y_test[i]:\n",
    "        fp += 1\n",
    "    if predictions[i] == 0 and predictions[i] != y_test[i]:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "precision = tp/(tp+fp)\n",
    "print('precision is',precision,'for 1')\n",
    "recall = tp/(tp+fn)\n",
    "print('recall is',recall,'for 1')\n",
    "# f-measure\n",
    "f_measure = 2*precision*recall/(precision+recall)\n",
    "print('f-measure is',f_measure,'for 1')\n",
    "print()\n",
    "precision2 = tn / (tn + fn)\n",
    "recall2 = tn / (tn + fp)\n",
    "f_2 = 2 * precision2 * recall2 / (precision2 + recall2)\n",
    "print('precision is', precision2,'for 0')\n",
    "print('recall is: ', recall2,'for 0')\n",
    "print('f-measure is', f_2,'for 0')\n",
    "print()\n",
    "print('After implemented L2 norm regularization, I did not see any change with the accuracy, precision, recall or f-measure.')\n",
    "print('They are the same as those of logistic regression with y-intercept and bold driver. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
